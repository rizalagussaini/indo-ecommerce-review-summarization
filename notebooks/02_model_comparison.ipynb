{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison for Indonesian E-commerce Review Summarization\n",
    "\n",
    "This notebook compares different instruction-tuned LLMs for summarizing Indonesian e-commerce reviews.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from indo_ecommerce_review_summarization.preprocessing import clean_text\n",
    "from indo_ecommerce_review_summarization.models import create_summarization_prompt\n",
    "from indo_ecommerce_review_summarization.evaluation import evaluate_predictions, format_rouge_scores\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "Let's prepare some sample reviews and reference summaries for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample reviews and reference summaries\n",
    "test_data = [\n",
    "    {\n",
    "        \"reviews\": [\n",
    "            \"Barang bagus banget, sesuai deskripsi. Pengiriman cepat, packing rapi. Seller responsif.\"\n",
    "        ],\n",
    "        \"reference\": \"Produk berkualitas dengan pengiriman cepat dan seller responsif.\"\n",
    "    },\n",
    "    {\n",
    "        \"reviews\": [\n",
    "            \"Kualitas produk oke sih, tapi pengiriman agak lama. Overall masih puas.\"\n",
    "        ],\n",
    "        \"reference\": \"Produk bagus namun pengiriman lambat.\"\n",
    "    },\n",
    "    {\n",
    "        \"reviews\": [\n",
    "            \"Harga murah, kualitas lumayan. Recommended buat budget terbatas.\"\n",
    "        ],\n",
    "        \"reference\": \"Harga terjangkau dengan kualitas yang baik.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(test_data)} test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Mistral-7B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indo_ecommerce_review_summarization.models import load_model\n",
    "\n",
    "# Load Mistral model\n",
    "mistral_model = load_model(\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    model_type=\"huggingface\",\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(\"Mistral model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries with Mistral\n",
    "mistral_predictions = []\n",
    "\n",
    "for example in test_data:\n",
    "    prompt = create_summarization_prompt(\n",
    "        reviews=example[\"reviews\"],\n",
    "        model_type=\"mistral\",\n",
    "        max_length=30\n",
    "    )\n",
    "    \n",
    "    summary = mistral_model.generate(\n",
    "        prompt,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    mistral_predictions.append(summary)\n",
    "    print(f\"Review: {example['reviews'][0]}\")\n",
    "    print(f\"Summary: {summary}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Compare model performance using ROUGE metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract references\n",
    "references = [example[\"reference\"] for example in test_data]\n",
    "\n",
    "# Evaluate Mistral\n",
    "mistral_scores = evaluate_predictions(\n",
    "    predictions=mistral_predictions,\n",
    "    references=references\n",
    ")\n",
    "\n",
    "print(\"Mistral-7B-Instruct Performance:\")\n",
    "print(format_rouge_scores(mistral_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Using Other Models\n",
    "\n",
    "You can easily swap in other models by changing the model name and prompt template.\n",
    "\n",
    "### Example with LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to use LLaMA model\n",
    "# llama_model = load_model(\n",
    "#     model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "#     model_type=\"huggingface\",\n",
    "#     load_in_4bit=True,\n",
    "#     torch_dtype=torch.float16\n",
    "# )\n",
    "\n",
    "# llama_predictions = []\n",
    "# for example in test_data:\n",
    "#     prompt = create_summarization_prompt(\n",
    "#         reviews=example[\"reviews\"],\n",
    "#         model_type=\"llama\",\n",
    "#         max_length=30\n",
    "#     )\n",
    "#     summary = llama_model.generate(prompt, max_new_tokens=100)\n",
    "#     llama_predictions.append(summary)\n",
    "\n",
    "# llama_scores = evaluate_predictions(llama_predictions, references)\n",
    "# print(\"LLaMA-2-7B Performance:\")\n",
    "# print(format_rouge_scores(llama_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = {\n",
    "    \"Model\": [\"Mistral-7B-Instruct\"],\n",
    "    \"ROUGE-1 F1\": [mistral_scores[\"rouge1\"][\"fmeasure\"]],\n",
    "    \"ROUGE-2 F1\": [mistral_scores[\"rouge2\"][\"fmeasure\"]],\n",
    "    \"ROUGE-L F1\": [mistral_scores[\"rougeL\"][\"fmeasure\"]],\n",
    "}\n",
    "\n",
    "# Uncomment to add more models\n",
    "# comparison_data[\"Model\"].append(\"LLaMA-2-7B\")\n",
    "# comparison_data[\"ROUGE-1 F1\"].append(llama_scores[\"rouge1\"][\"fmeasure\"])\n",
    "# comparison_data[\"ROUGE-2 F1\"].append(llama_scores[\"rouge2\"][\"fmeasure\"])\n",
    "# comparison_data[\"ROUGE-L F1\"].append(llama_scores[\"rougeL\"][\"fmeasure\"])\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. How to compare multiple LLMs for Indonesian review summarization\n",
    "2. Evaluation using ROUGE metrics\n",
    "3. A framework that's easy to extend to other models\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "- Test with larger datasets for more reliable comparisons\n",
    "- Experiment with different prompt templates\n",
    "- Consider human evaluation for quality assessment\n",
    "- Fine-tune models on Indonesian e-commerce data for better performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

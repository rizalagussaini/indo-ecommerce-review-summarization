{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE-Based Evaluation for Indonesian Review Summarization\n",
    "\n",
    "This notebook demonstrates how to evaluate summarization quality using ROUGE metrics.\n",
    "\n",
    "## What is ROUGE?\n",
    "\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics for evaluating automatic summarization:\n",
    "\n",
    "- **ROUGE-1**: Overlap of unigrams (single words)\n",
    "- **ROUGE-2**: Overlap of bigrams (two consecutive words)\n",
    "- **ROUGE-L**: Longest common subsequence\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from indo_ecommerce_review_summarization.evaluation import (\n",
    "    calculate_rouge,\n",
    "    evaluate_predictions,\n",
    "    format_rouge_scores\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Single Prediction Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction and reference\n",
    "prediction = \"Produk berkualitas baik dengan pengiriman cepat dan harga terjangkau.\"\n",
    "reference = \"Barang bagus, pengiriman cepat, harga murah.\"\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "scores = calculate_rouge(prediction, reference)\n",
    "\n",
    "print(\"Single Example Evaluation:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"Reference:  {reference}\")\n",
    "print(\"\\nROUGE Scores:\")\n",
    "print(format_rouge_scores(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Batch Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple predictions and references\n",
    "predictions = [\n",
    "    \"Produk bagus dengan pengiriman cepat.\",\n",
    "    \"Kualitas oke tapi pengiriman lambat.\",\n",
    "    \"Harga murah dan kualitas baik.\",\n",
    "    \"Seller responsif dan barang sesuai deskripsi.\",\n",
    "    \"Packing rapi, produk berkualitas tinggi.\"\n",
    "]\n",
    "\n",
    "references = [\n",
    "    \"Barang bagus, pengiriman sangat cepat.\",\n",
    "    \"Produk berkualitas namun pengiriman terlambat.\",\n",
    "    \"Harga terjangkau dengan kualitas yang bagus.\",\n",
    "    \"Seller sangat responsif, barang sesuai ekspektasi.\",\n",
    "    \"Kemasan rapi dan produk sangat berkualitas.\"\n",
    "]\n",
    "\n",
    "# Evaluate all predictions\n",
    "scores = evaluate_predictions(predictions, references)\n",
    "\n",
    "print(\"Batch Evaluation Results:\")\n",
    "print(\"=\"*80)\n",
    "print(format_rouge_scores(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Per-Example Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scores for each example\n",
    "per_example_scores = evaluate_predictions(\n",
    "    predictions,\n",
    "    references,\n",
    "    aggregate=False\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"Per-Example ROUGE Scores:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, (pred, ref, scores) in enumerate(zip(predictions, references, per_example_scores), 1):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Prediction: {pred}\")\n",
    "    print(f\"Reference:  {ref}\")\n",
    "    print(f\"ROUGE-1 F1: {scores['rouge1']['fmeasure']:.4f}\")\n",
    "    print(f\"ROUGE-2 F1: {scores['rouge2']['fmeasure']:.4f}\")\n",
    "    print(f\"ROUGE-L F1: {scores['rougeL']['fmeasure']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Analyzing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for analysis\n",
    "data = []\n",
    "for i, (pred, ref, scores) in enumerate(zip(predictions, references, per_example_scores), 1):\n",
    "    data.append({\n",
    "        \"Example\": i,\n",
    "        \"Prediction\": pred,\n",
    "        \"Reference\": ref,\n",
    "        \"ROUGE-1\": scores['rouge1']['fmeasure'],\n",
    "        \"ROUGE-2\": scores['rouge2']['fmeasure'],\n",
    "        \"ROUGE-L\": scores['rougeL']['fmeasure'],\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"Summary Statistics:\")\n",
    "print(\"=\"*80)\n",
    "print(df[['ROUGE-1', 'ROUGE-2', 'ROUGE-L']].describe())\n",
    "\n",
    "# Find best and worst examples\n",
    "print(\"\\nBest Example (by ROUGE-L):\")\n",
    "best_idx = df['ROUGE-L'].idxmax()\n",
    "print(f\"Prediction: {df.loc[best_idx, 'Prediction']}\")\n",
    "print(f\"Reference:  {df.loc[best_idx, 'Reference']}\")\n",
    "print(f\"ROUGE-L:    {df.loc[best_idx, 'ROUGE-L']:.4f}\")\n",
    "\n",
    "print(\"\\nWorst Example (by ROUGE-L):\")\n",
    "worst_idx = df['ROUGE-L'].idxmin()\n",
    "print(f\"Prediction: {df.loc[worst_idx, 'Prediction']}\")\n",
    "print(f\"Reference:  {df.loc[worst_idx, 'Reference']}\")\n",
    "print(f\"ROUGE-L:    {df.loc[worst_idx, 'ROUGE-L']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Comparing Different Summarization Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate different models/approaches\n",
    "approach1_predictions = [\n",
    "    \"Produk berkualitas dengan pengiriman cepat.\",\n",
    "    \"Kualitas baik namun pengiriman terlambat.\",\n",
    "    \"Harga murah kualitas oke.\",\n",
    "]\n",
    "\n",
    "approach2_predictions = [\n",
    "    \"Barang bagus, kirim cepat banget.\",\n",
    "    \"Produk oke, sayang pengirimannya lama.\",\n",
    "    \"Murah dan berkualitas.\",\n",
    "]\n",
    "\n",
    "test_references = [\n",
    "    \"Barang bagus, pengiriman sangat cepat.\",\n",
    "    \"Produk berkualitas namun pengiriman terlambat.\",\n",
    "    \"Harga terjangkau dengan kualitas yang bagus.\",\n",
    "]\n",
    "\n",
    "# Evaluate both approaches\n",
    "scores1 = evaluate_predictions(approach1_predictions, test_references)\n",
    "scores2 = evaluate_predictions(approach2_predictions, test_references)\n",
    "\n",
    "# Compare\n",
    "print(\"Approach Comparison:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Metric\": [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"],\n",
    "    \"Approach 1\": [\n",
    "        scores1['rouge1']['fmeasure'],\n",
    "        scores1['rouge2']['fmeasure'],\n",
    "        scores1['rougeL']['fmeasure']\n",
    "    ],\n",
    "    \"Approach 2\": [\n",
    "        scores2['rouge1']['fmeasure'],\n",
    "        scores2['rouge2']['fmeasure'],\n",
    "        scores2['rougeL']['fmeasure']\n",
    "    ]\n",
    "})\n",
    "\n",
    "comparison_df['Difference'] = comparison_df['Approach 1'] - comparison_df['Approach 2']\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting ROUGE Scores\n",
    "\n",
    "### General Guidelines:\n",
    "\n",
    "- **ROUGE-1**: Measures content overlap at word level\n",
    "  - Higher scores indicate better content coverage\n",
    "  - Typical range: 0.3-0.6 for good summaries\n",
    "\n",
    "- **ROUGE-2**: Measures phrase-level overlap\n",
    "  - More stringent than ROUGE-1\n",
    "  - Indicates better phrase matching\n",
    "  - Typical range: 0.15-0.4 for good summaries\n",
    "\n",
    "- **ROUGE-L**: Measures longest common subsequence\n",
    "  - Captures sentence-level structure\n",
    "  - Good indicator of fluency\n",
    "  - Typical range: 0.3-0.5 for good summaries\n",
    "\n",
    "### Important Notes:\n",
    "\n",
    "1. ROUGE is not perfect - high scores don't always mean better quality\n",
    "2. Human evaluation is still important\n",
    "3. Consider multiple metrics together\n",
    "4. Indonesian language has different characteristics than English\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Single and batch ROUGE evaluation\n",
    "2. Per-example analysis\n",
    "3. Comparing different approaches\n",
    "4. Interpreting ROUGE scores\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Evaluate your model on a test set\n",
    "- Compare with baseline models\n",
    "- Perform error analysis on low-scoring examples\n",
    "- Consider additional metrics (BLEU, METEOR, BERTScore)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
